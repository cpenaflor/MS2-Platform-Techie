{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b56b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns before normalization: ['week', 'avg_users', 'sales_growth_rate', 'col_4', 'col_5', 'col_6', 'col_7', 'col_8', 'col_9', 'col_10', 'col_11', 'col_12', 'col_13', 'col_14', 'col_15', 'col_16', 'col_17', 'col_18', 'col_19', 'col_20', 'col_21', 'col_22', 'col_23', 'col_24', 'col_25', 'col_26', 'col_27', 'col_28', 'col_29', 'col_30', 'col_31', 'col_32', 'col_33', 'col_34', 'col_35', 'col_36', 'col_37', 'col_38', 'col_39', 'col_40', 'col_41', 'col_42', 'col_43', 'col_44', 'col_45', 'col_46', 'col_47', 'col_48', 'col_49', 'col_50']\n",
      "Columns after normalization: ['week', 'avg_users', 'sales_growth_rate', 'col_4', 'col_5', 'col_6', 'col_7', 'col_8', 'col_9', 'col_10', 'col_11', 'col_12', 'col_13', 'col_14', 'col_15', 'col_16', 'col_17', 'col_18', 'col_19', 'col_20', 'col_21', 'col_22', 'col_23', 'col_24', 'col_25', 'col_26', 'col_27', 'col_28', 'col_29', 'col_30', 'col_31', 'col_32', 'col_33', 'col_34', 'col_35', 'col_36', 'col_37', 'col_38', 'col_39', 'col_40', 'col_41', 'col_42', 'col_43', 'col_44', 'col_45', 'col_46', 'col_47', 'col_48', 'col_49', 'col_50']\n",
      "Using date column: 'week'\n",
      "▶️ Raw data shape: (20, 50)\n",
      "Missing % per column:\n",
      "col_27               50.0\n",
      "col_24               50.0\n",
      "col_49               40.0\n",
      "col_21               35.0\n",
      "col_42               35.0\n",
      "col_41               35.0\n",
      "col_5                35.0\n",
      "col_39               30.0\n",
      "col_30               30.0\n",
      "col_43               30.0\n",
      "col_18               30.0\n",
      "col_17               25.0\n",
      "col_10               25.0\n",
      "col_4                25.0\n",
      "col_33               25.0\n",
      "col_48               25.0\n",
      "col_31               25.0\n",
      "col_15               20.0\n",
      "col_47               20.0\n",
      "col_50               20.0\n",
      "col_44               20.0\n",
      "col_37               20.0\n",
      "col_32               20.0\n",
      "col_23               20.0\n",
      "col_22               20.0\n",
      "col_20               20.0\n",
      "col_6                20.0\n",
      "col_8                20.0\n",
      "col_9                15.0\n",
      "col_13               15.0\n",
      "col_28               15.0\n",
      "col_19               15.0\n",
      "col_14               15.0\n",
      "col_7                15.0\n",
      "col_46               15.0\n",
      "col_35               15.0\n",
      "col_36               15.0\n",
      "col_38               15.0\n",
      "col_11               10.0\n",
      "col_16               10.0\n",
      "col_26               10.0\n",
      "col_29               10.0\n",
      "col_40               10.0\n",
      "col_34               10.0\n",
      "col_45               10.0\n",
      "col_12                5.0\n",
      "sales_growth_rate     0.0\n",
      "avg_users             0.0\n",
      "week                  0.0\n",
      "col_25                0.0\n",
      "dtype: float64\n",
      "After dropping cols, shape: (20, 3)\n",
      "After duplicate removal, shape: (20, 3)\n",
      "Numeric columns: ['avg_users', 'sales_growth_rate']\n",
      "Categorical columns: []\n",
      "After numeric & category clean, shape: (20, 3)\n",
      "⚠️ Skipping outlier filtering for trend report.\n",
      "✔️ Pandera validation passed\n",
      "✅ Cleaned data saved to: C:\\Users\\chang\\Desktop\\MS2 Platform Technology\\CLEANED_trend_report.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pandera as pa\n",
    "from pandera import Column, Check, DataFrameSchema\n",
    "\n",
    "# File paths\n",
    "base_dir = r\"C:\\Users\\chang\\Desktop\\MS2 Platform Technology\"\n",
    "raw_path = os.path.join(base_dir, \"trend_report.csv\")\n",
    "clean_path = os.path.join(base_dir, \"CLEANED_trend_report.csv\")\n",
    "\n",
    "# 1 Load raw data & normalize column names\n",
    "df = pd.read_csv(raw_path)\n",
    "print(\"Columns before normalization:\", df.columns.tolist())\n",
    "df.columns = (\n",
    "    df.columns\n",
    "      .str.strip()\n",
    "      .str.lower()\n",
    "      .str.replace(r\"[ \\-]+\", \"_\", regex=True)\n",
    ")\n",
    "print(\"Columns after normalization:\", df.columns.tolist())\n",
    "\n",
    "# 2 Detect date column\n",
    "def find_column(candidates, cols):\n",
    "    for c in candidates:\n",
    "        if c in cols:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "date_col = find_column([\"trend_date\", \"date\", \"report_date\", \"week\"], df.columns)\n",
    "if date_col is None:\n",
    "    raise KeyError(f\"Date column missing. Available columns: {df.columns.tolist()}\")\n",
    "print(f\"Using date column: '{date_col}'\")\n",
    "\n",
    "# 3 Initial shape and missing-value overview\n",
    "print(f\"▶️ Raw data shape: {df.shape}\")\n",
    "print(\"Missing % per column:\")\n",
    "print((df.isnull().mean() * 100).sort_values(ascending=False))\n",
    "\n",
    "# 4 Drop high-null & irrelevant columns\n",
    "high_null = df.columns[df.isnull().mean() > 0.5]\n",
    "pattern_cols = [c for c in df.columns if c.startswith(\"col_\")]\n",
    "# Keep date column\n",
    "drop_cols = [c for c in list(high_null) + pattern_cols if c != date_col]\n",
    "df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "print(f\"After dropping cols, shape: {df.shape}\")\n",
    "\n",
    "# 5 Remove duplicates\n",
    "df = df.drop_duplicates()\n",
    "print(f\"After duplicate removal, shape: {df.shape}\")\n",
    "\n",
    "# 6 Numeric & categorical fixes\n",
    "numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "cat_cols = [c for c in df.select_dtypes(include=[\"object\"]).columns if c != date_col]\n",
    "print(f\"Numeric columns: {numeric_cols}\")\n",
    "print(f\"Categorical columns: {cat_cols}\")\n",
    "# Numeric: cast to float (allow negatives)\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(float)\n",
    "# Categorical: strip, lower, cast to category\n",
    "for col in cat_cols:\n",
    "    df[col] = df[col].astype(str).str.strip().str.lower().astype(\"category\")\n",
    "print(f\"After numeric & category clean, shape: {df.shape}\")\n",
    "\n",
    "# 7 (Optional) Outlier filtering\n",
    "print(\"⚠️ Skipping outlier filtering for trend report.\")\n",
    "\n",
    "# 8 Define & apply Pandera schema\n",
    "schema_cols = { date_col: Column(str, nullable=False) }\n",
    "for col in numeric_cols:\n",
    "    schema_cols[col] = Column(float, nullable=True)\n",
    "for col in cat_cols:\n",
    "    schema_cols[col] = Column(pa.Category, nullable=True)\n",
    "schema = DataFrameSchema(schema_cols)\n",
    "df = schema.validate(df, lazy=True)\n",
    "print(\"✔️ Pandera validation passed\")\n",
    "\n",
    "# 9 Save cleaned data\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f\"✅ Cleaned data saved to: {clean_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e1b49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: All required columns present\n",
      "INFO: Schema validation passed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== trend_report Pipeline Health Report ===\n",
      "Missing/Broken columns detected: []\n",
      "Schema double check:             Passed\n",
      "Error Handling check:            Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import logging\n",
    "import pandera as pa\n",
    "from pandera import Column, Check, DataFrameSchema\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 1 Track any upstream errors\n",
    "# If you wrapped your pd.read_csv in try/except, set this accordingly; otherwise:\n",
    "error_handling = \"Passed\"\n",
    "\n",
    "# 2 Detect missing or broken columns\n",
    "# We require the date column plus all numeric metrics\n",
    "required = [date_col] + numeric_cols\n",
    "missing = [col for col in required if col not in df.columns]\n",
    "if missing:\n",
    "    report_missing = missing\n",
    "    logger.warning(f\"Required columns missing: {missing}\")\n",
    "else:\n",
    "    report_missing = []\n",
    "    logger.info(\"All required columns present\")\n",
    "\n",
    "# 3 Define Pandera schema for double‐check\n",
    "schema = DataFrameSchema({\n",
    "    date_col: Column(str, nullable=False),\n",
    "    **{col: Column(float, nullable=True) for col in numeric_cols}\n",
    "})\n",
    "\n",
    "# 4 Run schema validation, capture outcome\n",
    "if df.empty:\n",
    "    report_schema = \"Skipped (empty DataFrame)\"\n",
    "else:\n",
    "    try:\n",
    "        schema.validate(df, lazy=True)\n",
    "        report_schema = \"Passed\"\n",
    "        logger.info(\"Schema validation passed\")\n",
    "    except pa.errors.SchemaErrors as err:\n",
    "        report_schema = f\"Failed: {len(err.failure_cases)} errors\"\n",
    "        logger.error(\"Schema validation failed:\\n%s\", err.failure_cases)\n",
    "\n",
    "# 5 Print consolidated health report\n",
    "print(\"\\n=== trend_report Pipeline Health Report ===\")\n",
    "print(f\"Missing/Broken columns detected: {report_missing}\")\n",
    "print(f\"Schema double check:             {report_schema}\")\n",
    "print(f\"Error Handling check:            {error_handling}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
