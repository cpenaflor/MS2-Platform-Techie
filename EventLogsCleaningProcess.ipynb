{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba339184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns before normalization: ['user_id', 'event_type', 'event_time', 'product_id', 'amount', 'col_6', 'col_7', 'col_8', 'col_9', 'col_10', 'col_11', 'col_12', 'col_13', 'col_14', 'col_15', 'col_16', 'col_17', 'col_18', 'col_19', 'col_20', 'col_21', 'col_22', 'col_23', 'col_24', 'col_25', 'col_26', 'col_27', 'col_28', 'col_29', 'col_30', 'col_31', 'col_32', 'col_33', 'col_34', 'col_35', 'col_36', 'col_37', 'col_38', 'col_39', 'col_40', 'col_41', 'col_42', 'col_43', 'col_44', 'col_45', 'col_46', 'col_47', 'col_48', 'col_49', 'col_50']\n",
      "Columns after normalization: ['user_id', 'event_type', 'event_time', 'product_id', 'amount', 'col_6', 'col_7', 'col_8', 'col_9', 'col_10', 'col_11', 'col_12', 'col_13', 'col_14', 'col_15', 'col_16', 'col_17', 'col_18', 'col_19', 'col_20', 'col_21', 'col_22', 'col_23', 'col_24', 'col_25', 'col_26', 'col_27', 'col_28', 'col_29', 'col_30', 'col_31', 'col_32', 'col_33', 'col_34', 'col_35', 'col_36', 'col_37', 'col_38', 'col_39', 'col_40', 'col_41', 'col_42', 'col_43', 'col_44', 'col_45', 'col_46', 'col_47', 'col_48', 'col_49', 'col_50']\n",
      "Detected columns -> user: user_id, time: event_time, type: event_type, amount: amount\n",
      "▶️ Raw data shape: (2000, 50)\n",
      "Missing % per column:\n",
      "amount    50.80\n",
      "col_38    31.70\n",
      "col_50    31.50\n",
      "col_49    31.35\n",
      "col_28    31.00\n",
      "col_44    31.00\n",
      "col_10    30.75\n",
      "col_41    30.70\n",
      "col_25    30.65\n",
      "col_29    30.65\n",
      "dtype: float64\n",
      "After dropping cols, shape: (2000, 5)\n",
      "After timestamp & duplicate clean, shape: (2000, 5)\n",
      "After numeric & category clean, shape: (509, 5)\n",
      "After outlier filter, shape: (509, 5)\n",
      "✔️ Pandera validation passed\n",
      "✅ Cleaned data saved to: C:\\Users\\chang\\Desktop\\MS2 Platform Technology\\CLEANED_eventlog.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandera\\_pandas_deprecated.py:146: FutureWarning: Importing pandas-specific classes and functions from the\n",
      "top-level pandera module will be **removed in a future version of pandera**.\n",
      "If you're using pandera to validate pandas objects, we highly recommend updating\n",
      "your import:\n",
      "\n",
      "```\n",
      "# old import\n",
      "import pandera as pa\n",
      "\n",
      "# new import\n",
      "import pandera.pandas as pa\n",
      "```\n",
      "\n",
      "If you're using pandera to validate objects from other compatible libraries\n",
      "like pyspark or polars, see the supported libraries section of the documentation\n",
      "for more information on how to import pandera:\n",
      "\n",
      "https://pandera.readthedocs.io/en/stable/supported_libraries.html\n",
      "\n",
      "To disable this warning, set the environment variable:\n",
      "\n",
      "```\n",
      "export DISABLE_PANDERA_IMPORT_WARNING=True\n",
      "```\n",
      "\n",
      "  warnings.warn(_future_warning, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pandera as pa\n",
    "from pandera import Column, Check, DataFrameSchema\n",
    "\n",
    "# File paths\n",
    "base_dir = r\"C:\\Users\\chang\\Desktop\\MS2 Platform Technology\"\n",
    "raw_path = os.path.join(base_dir, \"event_logs.csv\")\n",
    "clean_path = os.path.join(base_dir, \"CLEANED_eventlog.csv\")\n",
    "\n",
    "# 1 Load raw data & inspect columns\n",
    "df = pd.read_csv(raw_path)\n",
    "print(\"Columns before normalization:\", df.columns.tolist())\n",
    "# Normalize column names: strip whitespace, lowercase, replace spaces & hyphens with underscores\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .str.replace(r\"[ \\-]+\", \"_\", regex=True)\n",
    ")\n",
    "print(\"Columns after normalization:\", df.columns.tolist())\n",
    "\n",
    "# 2 Identify key columns\n",
    "def find_column(candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "user_col = find_column([\"user_id\", \"userid\", \"user\"])\n",
    "time_col = find_column([\"event_time\", \"timestamp\", \"time\"])\n",
    "type_col = find_column([\"event_type\", \"eventtype\", \"type\"])\n",
    "amt_col  = find_column([\"amount\", \"amt\", \"value\"])\n",
    "\n",
    "print(f\"Detected columns -> user: {user_col}, time: {time_col}, type: {type_col}, amount: {amt_col}\")\n",
    "if not all([user_col, time_col, type_col]):\n",
    "    raise KeyError(\"Missing essential columns. Available columns: \" + \", \".join(df.columns))\n",
    "\n",
    "# 3 Initial shape and missing-value overview\n",
    "print(f\"▶️ Raw data shape: {df.shape}\")\n",
    "print(\"Missing % per column:\")\n",
    "print((df.isnull().mean() * 100).sort_values(ascending=False).head(10))\n",
    "\n",
    "# 4 Drop high-null & irrelevant columns\n",
    "high_null = df.columns[df.isnull().mean() > 0.5]\n",
    "pattern_cols = [c for c in df.columns if c.startswith(\"col_\")]\n",
    "drop_cols = list(high_null) + pattern_cols\n",
    "# Ensure we don't drop essential cols by accident\n",
    "drop_cols = [c for c in drop_cols if c not in [user_col, time_col, type_col, amt_col]]\n",
    "df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
    "print(f\"After dropping cols, shape: {df.shape}\")\n",
    "\n",
    "# 5 Parse & clean key fields\n",
    "df[time_col] = pd.to_datetime(df[time_col], errors=\"coerce\")\n",
    "df = df.dropna(subset=[user_col, time_col, type_col])\n",
    "df = df.drop_duplicates()\n",
    "print(f\"After timestamp & duplicate clean, shape: {df.shape}\")\n",
    "\n",
    "# 6 Numeric & categorical fixes\n",
    "if amt_col:\n",
    "    df[amt_col] = pd.to_numeric(df[amt_col], errors=\"coerce\")\n",
    "    df = df[df[amt_col].ge(0)]\n",
    "# Standardize event_type\n",
    "allowed = [\"page_view\", \"checkout\", \"wishlist_add\", \"profile_update\"]\n",
    "df[type_col] = df[type_col].str.strip().str.lower()\n",
    "df = df[df[type_col].isin(allowed)]\n",
    "df[type_col] = df[type_col].astype(\"category\")\n",
    "print(f\"After numeric & category clean, shape: {df.shape}\")\n",
    "\n",
    "# 7 Outlier filtering on amount (1.5×IQR)\n",
    "if amt_col:\n",
    "    q1, q3 = df[amt_col].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "    df = df[df[amt_col].between(lower, upper)]\n",
    "    print(f\"After outlier filter, shape: {df.shape}\")\n",
    "\n",
    "# 8 Pandera schema validation\n",
    "schema_cols = {\n",
    "    user_col: Column(str, nullable=False),\n",
    "    time_col: Column(pa.DateTime, nullable=False),\n",
    "    type_col: Column(pa.Category, Check.isin(allowed)),\n",
    "}\n",
    "if amt_col:\n",
    "    schema_cols[amt_col] = Column(float, Check.ge(0), nullable=True)\n",
    "schema = DataFrameSchema(schema_cols)\n",
    "df = schema.validate(df, lazy=True)\n",
    "print(\"✔️ Pandera validation passed\")\n",
    "\n",
    "# 9 Save cleaned data\n",
    "df.to_csv(clean_path, index=False)\n",
    "print(f\"✅ Cleaned data saved to: {clean_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e211f8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loaded raw data with shape (2000, 50)\n",
      "INFO: All required columns present\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Schema validation failed:\n",
      "    schema_context      column  \\\n",
      "0           Column  event_type   \n",
      "667         Column  event_type   \n",
      "654         Column  event_type   \n",
      "655         Column  event_type   \n",
      "656         Column  event_type   \n",
      "..             ...         ...   \n",
      "336         Column  event_type   \n",
      "337         Column  event_type   \n",
      "338         Column  event_type   \n",
      "339         Column  event_type   \n",
      "993         Column  event_type   \n",
      "\n",
      "                                                 check check_number  \\\n",
      "0    isin(['page_view', 'checkout', 'wishlist_add',...            0   \n",
      "667  isin(['page_view', 'checkout', 'wishlist_add',...            0   \n",
      "654  isin(['page_view', 'checkout', 'wishlist_add',...            0   \n",
      "655  isin(['page_view', 'checkout', 'wishlist_add',...            0   \n",
      "656  isin(['page_view', 'checkout', 'wishlist_add',...            0   \n",
      "..                                                 ...          ...   \n",
      "336  isin(['page_view', 'checkout', 'wishlist_add',...            0   \n",
      "337  isin(['page_view', 'checkout', 'wishlist_add',...            0   \n",
      "338  isin(['page_view', 'checkout', 'wishlist_add',...            0   \n",
      "339  isin(['page_view', 'checkout', 'wishlist_add',...            0   \n",
      "993                                  dtype('category')         None   \n",
      "\n",
      "    failure_case index  \n",
      "0          login     5  \n",
      "667  add_to_cart  1372  \n",
      "654        login  1336  \n",
      "655       search  1338  \n",
      "656  add_to_cart  1344  \n",
      "..           ...   ...  \n",
      "336       search   692  \n",
      "337  add_to_cart   693  \n",
      "338       search   695  \n",
      "339       logout   696  \n",
      "993       object  None  \n",
      "\n",
      "[994 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Pipeline Health Report ===\n",
      "Missing/Broken columns detected: []\n",
      "Schema double check: Failed: 994 errors\n",
      "Error Handling check: Passed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pandera as pa\n",
    "from pandera import Column, Check, DataFrameSchema\n",
    "\n",
    "# Setup minimal logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# File paths\n",
    "base_dir   = r\"C:\\Users\\chang\\Desktop\\MS2 Platform Technology\"\n",
    "raw_path   = os.path.join(base_dir, \"event_logs.csv\")\n",
    "\n",
    "# 1) Load data with error handling\n",
    "try:\n",
    "    df = pd.read_csv(raw_path)\n",
    "    logger.info(f\"Loaded raw data with shape {df.shape}\")\n",
    "    error_handling = \"Passed\"\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load raw data: {e}\")\n",
    "    df = pd.DataFrame()\n",
    "    error_handling = f\"Failed ({e})\"\n",
    "\n",
    "# Prepare report dict\n",
    "report = {\n",
    "    \"Missing/Broken columns detected\": None,\n",
    "    \"Schema double check\": None,\n",
    "    \"Error Handling check\": error_handling\n",
    "}\n",
    "\n",
    "# 2) Detect missing or broken columns\n",
    "required = [\"user_id\", \"event_time\", \"event_type\", \"amount\"]\n",
    "missing = [col for col in required if col not in df.columns]\n",
    "if missing:\n",
    "    report[\"Missing/Broken columns detected\"] = missing\n",
    "    logger.warning(f\"Required columns missing: {missing}\")\n",
    "else:\n",
    "    report[\"Missing/Broken columns detected\"] = []\n",
    "    logger.info(\"All required columns present\")\n",
    "\n",
    "# 3) Define Pandera schema for double-check\n",
    "schema = DataFrameSchema({\n",
    "    \"user_id\":    Column(str, nullable=False),\n",
    "    \"event_time\": Column(pa.DateTime, nullable=False),\n",
    "    \"event_type\": Column(pa.Category, Check.isin([\"page_view\",\"checkout\",\"wishlist_add\",\"profile_update\"])),\n",
    "    \"amount\":     Column(float, Check.ge(0), nullable=True),\n",
    "})\n",
    "\n",
    "# 4) Run schema validation, capture outcome\n",
    "if not df.empty and not missing:\n",
    "    try:\n",
    "        schema.validate(df, lazy=True)\n",
    "        report[\"Schema double check\"] = \"Passed\"\n",
    "        logger.info(\"Schema validation passed\")\n",
    "    except pa.errors.SchemaErrors as err:\n",
    "        report[\"Schema double check\"] = f\"Failed: {len(err.failure_cases)} errors\"\n",
    "        logger.error(\"Schema validation failed:\\n%s\", err.failure_cases)\n",
    "else:\n",
    "    report[\"Schema double check\"] = \"Skipped\"\n",
    "\n",
    "# 5) Print consolidated report\n",
    "print(\"\\n=== Pipeline Health Report ===\")\n",
    "for k, v in report.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
